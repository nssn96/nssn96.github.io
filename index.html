<!DOCTYPE html>
<html lang="en">

  <head>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Surya's portfolio</title>

    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom fonts for this template -->
    <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:100,200,300,400,500,600,700,800,900" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i,800,800i" rel="stylesheet">
    <link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet">
    <link href="vendor/devicons/css/devicons.min.css" rel="stylesheet">
    <link href="vendor/simple-line-icons/css/simple-line-icons.css" rel="stylesheet">

    

    <!-- Custom styles for this template -->
    <link href="css/resume.min.css" rel="stylesheet">

  </head>

  <body id="page-top">

    <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
      <a class="navbar-brand js-scroll-trigger" href="#page-top">
        <span class="d-block d-lg-none">Start Bootstrap</span>
        <span class="d-none d-lg-block">
          <img class="img-fluid img-profile rounded-circle mx-auto mb-2" src="img/Profilepic.jpg" alt="">
        </span>
      </a>
      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse" id="navbarSupportedContent">
        <ul class="navbar-nav">
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#about">About</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#experience">Experience</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#education">Education</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#skills">Skills</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#projects">Projects</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#awards">Achievements & Training</a>
          </li>

          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#blogs">Blogs</a>
          </li>
    
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="https://drive.google.com/file/d/10FLS1-qCun7MglqPrRWKn5vYn93fLBrn/view?usp=sharing">Resume</a>
          </li>


        </ul>
      </div>
    </nav>

    <div class="container-fluid p-0">

      <section class="resume-section p-3 p-lg-5 d-flex d-column" id="about">
        <div class="my-auto">
          <h1 class="mb-0">Surya
            <span class="text-primary">Narayanan N S</span>
          </h1>

         

          <!-- Header Headlines -->
          <p class='hero-subtitle'>
            I am a <span class='typist-text typist-blink' data-typist='ML Enthusiast,Amateur Photographer' id='typist-element'>
              Code Lover, AI Enthusiast, Electronics Engineer</span>
          </p>

          <div class="subheading mb-5">Arlington,TX--(214) 929-0233--
            <a href="mailto:name@email.com">nssurya1996@gmail.com</a>
          </div>
          <p class="mb-5">I am a Software Developer with experience in SOA architecture and JAVA with an interest to learn and master Software Development, Robotics and AI.
             Looking for an opportunity where I can use my knowledge and understanding from my EIE undergrad to focus on the application of AI methodologies for solving real-world problems in the electronics field.</p>
          <ul class="list-inline list-social-icons mb-0">
           <!---- <li class="list-inline-item">
              <a href="https://www.facebook.com/profile.php?id=100006363160192">
                <span class="fa-stack fa-lg">
                  <i class="fa fa-circle fa-stack-2x"></i>
                  <i class="fa fa-facebook fa-stack-1x fa-inverse"></i>
                </span>
              </a>
            </li> -->
            
            <li class="list-inline-item">
              <a href="https://www.linkedin.com/in/surya-narayanan-nadhamuni-suresh-bb1686124?lipi=urn%3Ali%3Apage%3Ad_flagship3_profile_view_base_contact_details%3BsLNXNBS9QbKaid4vslazGQ%3D%3D">
                <span class="fa-stack fa-lg">
                  <i class="fa fa-circle fa-stack-2x"></i>
                  <i class="fa fa-linkedin fa-stack-1x fa-inverse"></i>
                </span>
              </a>
            </li>
            <li class="list-inline-item">
              <a href="https://github.com/nssn96">
                <span class="fa-stack fa-lg">
                  <i class="fa fa-circle fa-stack-2x"></i>
                  <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                </span>
              </a>
            </li>
          </ul>
        </div>
      </section>

      <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="experience">
        <div class="my-auto">
          <h2 class="mb-5">Experience</h2>

          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Programmer Analyst</h3>
              <div class="subheading mb-3">Cognizant Technology Solutions India</div>
              <div class="subheading mb-3">Project Name</div>
              <p>Schneider National, US -- L2 Support and Enhancements Project for Schneider Fleet management System (Integration and Telematics (IoT Solutions))</p> 
              <div class="subheading mb-3">Project Description </div>
                <p>
                Schneider Asset and Non-Asset IT Landscape comprises various Oracle Solutions (OTM, B2B, BI, EBS, HCM) and these oracle applications are integrated through various Oracle Fusion Middleware products such as OSB, SOA, ODI, ADF. Schneider Next-generation Fleet management System IT landscape is built on AWS which is being utilized to develop IoT solutions. IoT Solutions which are built to provide seamless business are Platform Science, Orion, Kepler, Kabana and other ELK stack solutions 
                </p>
                <div class="subheading mb-3">Role & Responsibilities</div>
                <p>• Worked extensively in ODI/SOA/OSB Migration From 11g to 12c.</br>
                  <br>•	Supporting critical production activities and monitoring.</br>
                  <br>•	Provide a real-time solution to the issues faced in the Telematics region by interacting with the drivers and going through the IoT platforms (Orion, Platform science, Kibana, Kepler).</br>   
                  <br>•	Monitor business transactions end to end in FMW consisting of OSB, SOA, ODI.</br>
                  <br> •	Performing load testing and analysis of performance in lower MW environments</br> 
                  <br>•	Understanding of business impacts, SLA </br>
                  <br>•	Log/RCA analysis in case of production issues</br> 
                  <br>•	OSB/ODI enhancements according to the new business requirement or as a part of bug fixing.</br> 
                  <br>•	Responsible for automation of job monitoring reports sent as a part of the production support activity.</br> 
                  </p>
            </div>
            <div class="resume-date text-md-right">
              <span class="text-primary">November 2017-December 2020</span>
            </div>
          </div>

      

        </div>

      </section>

      <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="education">
        <div class="my-auto">
          <h2 class="mb-5">Education</h2>

          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">UNIVERSITY OF TEXAS AT ARLINGTON</h3>
              <div class="subheading mb-3">Master of Science in Computer Science-Pursuing</div>
              <p>Relevant Coursework:</br>
                •	Design & Analysis of Algorithms</br>
                •	Data Mining</br>
                •	Web Data management </br>
                </p>
            </div>
            <div class="resume-date text-md-right">
              <span class="text-primary">Jan 2021 - May 2023</span>
            </div>
          </div>

          <div class="resume-item d-flex flex-column flex-md-row">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Amrita School of Engineering, Bengaluru</h3>
              <div class="subheading mb-3">Bachelor of Technology in Electronics and Instrumentation Engineering</div>
              <p>GPA: 7.66</p>
              <p>
                Relevant Coursework</br>
                •	Digital Systems</br>
                •	Digital Signal Processing</br>
                •	Embedded Systems</br>
                •	Robotic Control</br>
                •	Pattern Recognition Techniques and Algorithms</br>
                •	IT Essentials</br>
                •	Econometrics</br>
                </p>
            </div>
            <div class="resume-date text-md-right">
              <span class="text-primary">August 2013 - May 2017</span>
            </div>
          </div>

        </div>
      </section>

      <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="skills">
        <div class="my-auto">
          <h2 class="mb-5">Skills</h2>

          <div class="subheading mb-3">Programming Languages</div>
          <p>C, C++, Java , XML , HTML,CSS , Python(basic)</p>

          <div class="subheading mb-3">Software tools</div>
          <p>Arduino , MATLAB , LABVIEW</p>

          <div class="subheading mb-3">Architecture</div>
          <p>SOA(11g/12c) , Microservices , MVC</p>

          <div class="subheading mb-3">Database </div>
          <p>MySQL , PL SQL</p>

          
          
          <!--<ul class="list-inline list-icons">
            <li class="list-inline-item">
              <i class="devicons devicons-html5"></i>
            </li>
            <li class="list-inline-item">
              <i class="devicons devicons-css3"></i>
            </li>
            <li class="list-inline-item">
              <i class="devicons devicons-javascript"></i>
            </li>
            <li class="list-inline-item">
              <i class="devicons devicons-jquery"></i>
            </li>
            <li class="list-inline-item">
              <i class="devicons devicons-sass"></i>
            </li>
            <li class="list-inline-item">
              <i class="devicons devicons-less"></i>
            </li>
            <li class="list-inline-item">
              <i class="devicons devicons-bootstrap"></i>
            </li>
            <li class="list-inline-item">
              <i class="devicons devicons-wordpress"></i>
            </li>
            <li class="list-inline-item">
              <i class="devicons devicons-grunt"></i>
            </li>
            <li class="list-inline-item">
              <i class="devicons devicons-gulp"></i>
            </li>
            <li class="list-inline-item">
              <i class="devicons devicons-npm"></i>
            </li>
          </ul> -->

          
            
            
          </ul>
        </div>
      </section>

      <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="projects">
        <div class="my-auto">
          <h2 class="mb-5">Relevant Projects</h2>

          <div class="subheading mb-3">1) Diabetes classifier</div>
          <p>This Application is used for defining a pattern and classify whether a female is Diabetic or non-diabetic.</p>
          <p>The link for the <a href="https://drive.google.com/file/d/1G_-_UTyOiRurAwQC99fuklbEGqvuIaLI/view?usp=sharing">Project Proposal</a></p>

          <div class="subheading mb-3">Why will someone need this app?</div>
          <p>This Application is used for defining a pattern and classify whether a female is Diabetic or non-diabetic. 
            Diabetes is a very issue in our generation. Diabetes, especially type 2, is more common in males rather than females. 
            However, females often have more serious complications and a greater risk of death. So, if we are able to diagnose at an early stage,
             we can make some 
            lifestyle changes in order to minimize the long-term effects of Diabetes and stay healthy without any fatal effects.</p>

            <p>Here are some of the screenshots of the Diabchq app which runs based on the classifier I built</p>
            <p>This would be the login page once you have downloaded the app</p>
            <img  src="img/login.PNG"> <br><br>
            <p>These are the set of input features that will be required for the classifier to work</p>
            <img  src="img/inputpage.PNG"> <br><br>
            <p>This would be the Result page if the classifier predicts that the user has Diabetes</p>
            <img  src="img/diab.PNG"> <br><br>
            <p>This would be the Result page if the classifier predicts that the user is non-Diabetic</p>
            <img  src="img/nondiab.PNG"> <br><br>





          <div class="subheading mb-3">2) Immigrants Haven</div>
          <p>Programming Language : HTML,CSS,Javascript,PHP</p>
           <p>This is currently an ongoing project. Please find below the Homepage link and the project info</p>
          <p><a href="http://dxm4872.uta.cloud/WDM_project/php/homepage.php">Homepage</a></p>
          <p>The objective of this project is to create a website which will be a beneficial forum for all the immigrants moving to a new country. The website will connect them together so they can share their experiences, tips and will have access to a wealth of information like the cultural difference, rules and laws of that country and such. There will be three roles for this website - super admin, Admin per country, Immigrant. The super admin is the super user of the website. It can view, update and manage all the content on the website and assign/change the role of Admin per country. The Admin per country will oversee creating and managing all the content for a specific country. The Immigrant role is the immigrant user who creates a new profile on the website.</p>
          <p>HTML, CSS and JavaScript is used as the client-side scripting language. HTML is used to create the webpage. CSS helps in adding styles and design to the webpage. JavaScript is used for clicking events (Buttons) for communicating with the sever side. MySQL is used for the database purpose, stored in UTA Cloud. It stores the information as tables which are entered by the users on the screen. MySQL database communicates with the server on click events. The WordPress is used for creating the blog and the chat option will be created with the help of Node. Laravel is a free open-source PHP framework used for implementing the website.</p>
          <div class="subheading mb-3">3) Library Management System </div>
          <p>Programming Language : C++</p>
          <p>In this Library Management System project, you can enter the record of new books and retrieve the details of books available in the library. You can issue the books to the students and maintain their records. Late fine is charged for students who returns the issued books after the due date.Only one book is issued to students. New book is not issued to students those not returned the last book.</p>
         
          <div class="subheading mb-3">4) GARAGE OPEN-CLOSE SYSTEM </div>
          <p>Software : LABVIEW</p>
          <p>We operate the garage door using a passcode. The username and password are entered, it is compared with the authentic one in the code, and if the input is correct, the garage door automatically opens. If any input data is wrong, the access is denied. I learned from this application that there is extensive integration between computer programming and electronics, where we can use the LABVIEW software for many applications such as automating any process, proof of concept before building the electronic circuits, developing R&D service and testing tools for hardware products.
          </p>

          <a href="https://ieeexplore.ieee.org/document/8091338/metrics#metrics" <div class="subheading mb-3">5) A study of dealing serially correlated data in Gross Error Detection (GED) techniques</div></a>
          <p>Software : MATLAB</p>
          <p>Data validation and reconciliation (DVR) plays key role in industries because it uses process information and statistical methods to estimate correct measurements from the observed data. DR is very effective when measurement is free from gross error. Data observed from chemical processes can be serially correlated. Serial correlation in data can arise when a process, takes time to adjust, is exposed to prolonged influences, when data is manipulated or smoothened. If serial correlation is not taken care of, then gross error detection may be inaccurate. Hence dealing serial correlation is important in gross error detection (GED) techniques. Most of the techniques implemented in gross error detection require no correlation in the measurement. But experimental data may have serial correlation. In this project, various approaches like variance correction and pre-whitening are implemented to deal serial correlation on ARMA process and measurement test (MT) is applied to detect gross error. Results portray that MT is not an efficient method for GED and amongst the above mentioned two methods, pre-whitening with low variance is better than variance correction method.</p>
        
        
        
        
        
        </div>
      </section>

      <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="awards">
        <div class="my-auto">
          <h2 class="mb-5">Achievements &amp; Training</h2>
          <ul class="fa-ul mb-0">
            <div class="subheading mb-3">Achievements </div>
            <p>• Published a paper in IEEE International Conference on Signal Processing, Informatics, Communication and Energy Systems (SPICES), 2017 on the topic "A study of dealing serially correlated data in Gross Error Detection (GED) techniques"</br>
              •	Participated in the International Collegiate Programming Contest (ICPC), conducted by ACM in 2015.</br>
              
              •	Ranked fourth in the National level Robotics competition - ‘RACE 2014’ in the event “Line Follower” held at Amrita School of Engineering, Bangalore</br>
              •	Participated in LIFO (line follower) and Robowars in RASE’14 conducted by ACROM, Amrita School of Engineering, Bangalore.</br>
              
              •	 Participated Roboslam and LIFO (line follower) in SENTIENCE’2013 conducted by ACROM, Amrita School of Engineering, Bangalore.</br>
              
              •	Secured first place in our inter-department Volleyball tournament in AMRITA college.</br>	
              
              •	A volunteer in OUTREACH social welfare programs in COGNIZANT</br>  </p>

              <div class="subheading mb-3">Training </div>
              <p>• Attended the BASIC ROBOTICS HANDS-ON WORKSHOP conducted by ACROM, Amrita School of Engineering, Bangalore during the academic year 2013.</br>
                •	Participated in VOIP CONTROLLED ROBOTICS workshop conducted by i3indya technologies at ASE, Bangalore on 22nd and 23rd February of 2014.</br>
                •	Attended industrial automation training on Horner PLC conducted by AGIIT.</br>
                
                </p>
            
             
          </ul>
        </div>
      </section>

      <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="blogs">
        <div class="my-auto">
          <h2 class="mb-5">BLOGS</h2>
          <ul class="fa-ul mb-0">

            <h3 class="mb-0">BLOG 5 - Diabetes Classifier based on Naive bayes </h3></br>
            <p>In this blog we will be learning about the Diabetes Classifier that I built based on Naive Bayes Classifier.</p>
              <p>The link to my complete code in <a href="https://github.com/nssn96/Diabetes-Classifier/blob/main/DiabetesClassifier_Final.ipynb">Google colab</a></p>
              <div class="subheading mb-3">NBC-Introduction</div>
              <p>Naive Bayes classifier is classification algorithm based on Bayes Theorem.Bayes’ Theorem finds the probability of an event occurring given the probability of another event that has already occurred.</p>
              <div class="subheading mb-3">About Dataset</div>
              <p>I am importing the dataset directly from the Kaggle webbsite.This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases.
              The objective is to predict based on diagnostic measurements whether a patient has diabetes.</p>
              <p>Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old.</p>
              <p>1) Pregnancies: Number of times pregnant</br>
                2) Glucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test</br>
                3) BloodPressure: Diastolic blood pressure (mm Hg)</br>
                4) SkinThickness: Triceps skin fold thickness (mm)</br>
                5) Insulin: 2-Hour serum insulin (mu U/ml)</br>
                6) BMI: Body mass index (weight in kg/(height in m)^2)</br>
                7) DiabetesPedigreeFunction: Diabetes pedigree function</br>
                8) Age: Age (years)</br>
                9) Outcome: Class variable (0 for Non-Diabetic or 1 for Diabetic)</p>

                <div class="subheading mb-3"> Importing the required Libraries</div>
                <img  src="img/4import.PNG"> <br><br>
                
                <div class="subheading mb-3">Load and review data</div>
                <img  src="img/4load.PNG"> <br><br>

                <p> From the above output we can see that there are 2000 rows and 9 columns. This is what can be called as  Curse of dimensionality</p>
                <p>CURSE OF DIMENSIONALITY : Machine learning excels at analyzing data with many dimensions, but it becomes more challenging to create meaningful models as the number of rows is a way higher than the number of columns</p>
                 
                <div class="subheading mb-3">Statistical details of the dataset</div>
                <p> we can use the .describe() for that</p>
                <img  src="img/4stat.PNG"> <br><br>

                <p>Now, to check the different features of the dataset and the datatypes of it, we can use .info()</p>
                <img  src="img/4info.PNG"> <br><br>
                <p>We also should check if there are any null values in the dataset for the respective features</p>
                <img  src="img/4null.PNG"> <br><br>
                <p>From the above output we can see that there are no missing values in the dataset.We can consider the values zero in features as missing values and should be imputed before applying it to the classifier.Because Naive Bayes classifier is susceptible to missing values.</p>
                
                <div class="subheading mb-3">Exploratory Data Analysis</div>
                <p>Histogram</p>
                <img  src="img/4hist.PNG"> <br><br>
                <p>The following can be inferred from the above histogram</br>
                  1) Number of pregnancies is a discrete variable and it is left skewed.</br>
                  2) Glucose ,Blood pressure and BMI features are continuous variables and are normally distributed, so it will work well with naive bayes algorithm.</br>
                  3) Diabets Pedigree function feature is left skewed.</br>
                  4) Age feature is left skewed and is more of a discrete variable</p>
                 
                  <div class="subheading mb-3">To identify Correlation in data</div>
                  <p>This step is done inorder to identify dependency or correlation between the independent features. Naive Bayes classifier assumes that the predictor variables are mutually independent of each other ie there should no or less correlation between the independent variables.</p>
                  <img  src="img/4corr.PNG"> <br><br>
                  <p>However we want to see correlation in graphical representation so we can use Heatmap function for detecting the correlation</p>
                  <p>HeatMap</p>
                  <img  src="img/4corrheatmap.PNG"> <br><br>
                  <p>From the above we can see that most of the features are not correlated but Insulin and skin thickness,Age and Pregnancies have high positive correlation</p>
  
                  <p>Pairplot - to detect the correlation between independent variable using graphs or plots</p>
                  <img  src="img/4pp1.PNG"> <br><br>
                  <img  src="img/4pp2.PNG"> <br><br>
                  <p>From the above graph we can conform that Insulin and skin thickness, Age and Pregnancies have high positive correlation or highly correlated. The Problem of correlation between independent variables can be overcomed by feature Engineering.</p>
                  <div class="subheading mb-3">Checking for Outliers</div>
                  <p>In this step we will be checking for outliers in the dataset for the each individual features</p>
                  <img  src="img/4outp.PNG"> <br><br>
                  <img  src="img/4outgluc.PNG"> <br><br>
                  <img  src="img/4outbp.PNG"> <br><br>
                  <img  src="img/4outskin.PNG"> <br><br>
                  <img  src="img/4outinsulin.PNG"> <br><br>
                  <img  src="img/4outbmi.PNG"> <br><br>
                  <img  src="img/4outpedigree.PNG"> <br><br>
                  <img  src="img/4outage.PNG"> <br><br>
                  <p>From the above box-plotes we can see that all the features have outliers in them. The black dots in each box-plots are
                    the outliers
                  </p>

                  <p>Inorder to improve the model performance and make it stable, we need to remove the outliers which is what we will be doing in this next step</p>
                  <div class="subheading mb-3">Outlier removal
                  </div>
                  <p>We will be using the function remove_outlier for the removal of outliers</p>
                  <img  src="img/4outremoval.PNG"> <br><br>
                  <p>We will be plotting the box plot again to check whether the outliers were removed properly</p>
                  <img  src="img/4boxcheck.PNG"> <br><br>
                  <img  src="img/4box1.PNG"> <br><br>
                  <img  src="img/4box2.PNG"> <br><br>
                  <img  src="img/4box3.PNG"> <br><br>
                  <img  src="img/4box4.PNG"> <br><br>
                  <p>From the above box plots we can see that all the outliers have been removed successfully</p>

                  <div class="subheading mb-3">Checking for zeros in the dataset</div>
                  <p>Data Preparation-Check hidden missing values As we checked missing values earlier but haven't got any. But there can be lots of entries with 0 values. We must need to take care of those as well since
                    Naive Bayes classifier is very sensitive to zero values in dataset.
                  </p>
                  <img  src="img/4zero.PNG"> <br><br>
                  <p>From the above output we can see that there are zero values in number of Pregnancies, SkinThickness and Insulin. But, logically the number of pregnancies can be zero whereas the skinthickness and insulin cannot have the value zero and therefore needs to be imputed with median value.</p>
                  <p>The code for replacing the empty values with median value</p>
                  <img  src="img/4median.PNG"> <br><br>
                  <img  src="img/4medianout.PNG"> <br><br>
                  <p>From the above output we can see that except the number of pregnancies feature we dont have any zeros in the other fetaure</p>
                  <div class="subheading mb-3">Calculate diabetes ratio of True/False from outcome variable</div>
                  <img  src="img/4diabetesratio.PNG"> <br><br>
                  <p>So we have 34.20% people in current data set who have diabetes and rest of 65.80% doesn't have diabetes.</p>
                  <p>There is a slight data imbalance in the dataset which can be overcomed by SMOTE technique, which we can try it on experimentation section</p>
                 
                  <div class="subheading mb-3">Spliting the dataset</div>
                  <p>X indicates set of independent variables and Y indicates dependent variable. I will use 70% of data for training and 30% for testing.</p>
                  <p>we are using sklearn model selection to split the dataset into train and test set</p>
                  <img  src="img/4split.PNG"> <br><br>
                  <p>Lets check split of data</p>
                  <img  src="img/4split2.PNG"> <br><br>
                  <p>Now lets check diabetes True/False ratio in split data</p>
                  <img  src="img/4truesplit.PNG"> <br><br>


                  <div class="subheading mb-3">Train Naive Bayes algorithm</div>
                  <img  src="img/4naivealgo.PNG"> <br><br>
                  <div class="subheading mb-3">Probablity of occurance</div>
                  <p>The Probability of outcome in the Naive bayes model where if the probability value of zero occuring is higher the prediction will be 0 -> is non-diabetic and if Probability of 1 occuring is higer then the outcome will be 1 -> is Diabetic.</p>
                  <img  src="img/4testpredict.PNG"> <br><br>
                  <p>From the above Probability output in the first column 0.601113 > 0.398887 , therefor the Outcome of the prediction will be 1, as you can see which is the same from the below output</p>

                  <p>The outcome of the Naive bayes classifier.</p>
                  <img  src="img/4naiveoutcome.PNG"> <br><br>
                  <div class="subheading mb-3">Lets check the confusion matrix and classification report</div>
                  <p>Performance Matrix on train data set</p>
                  <img  src="img/4performtrain.PNG"> <br><br>
                  <p>The Recall and precision score indicate that it is a good model for the diabetes dataset</p>
                  <p>So, the total number of correct predictions done by the model = 754+311 = 1065</p>
                  <p>So, the total number of incorrect predictions done by the model = 158+177 = 335</p>
                  <p>Since the number of correct predictions is higher than the incorrect predictions the model works well in the training data</p>
                  <p>Performance Matrix on test data set</p>
                  <img  src="img/4performtest.PNG"> <br><br>
                  <p>The Recall and precision score indicate that it is a good model for the diabetes dataset</p>
                  <p>So, the total number of correct predictions done by the model = 324+132 = 456 </p>
                  <p>So, the total number of incorrect predictions done by the model = 80+64 = 144</p>
                  <p>Since the number of correct predictions is higher than the incorrect predictions the model works well in the test data</p>
                  <p>
                    From the above output from the performance of the model from training and testing dataset, we can say that the model has no overfitting or underfitting</p>

                    <div class="subheading mb-3">EXPERIMENT SECTION</div>
                    <p>Earlier we saw that there was a slight data imbalance in the dataset which might affect the accuracy of the model. This can be overcomed by SMOTE technique.</p>
                    <p>SMOTE--> Synthetic Minority Oversampling (SMOTE) works by creating synthetic observations based upon the existing minority observations</p>
                    <p>we are importing the SMOTE function and resampling x and y value</p>
                    <img  src="img/4smote.PNG"> <br><br>
                    <img  src="img/4xresample.PNG"> <br><br>
                    <p>we will then train the resampled x value with naive Bayes</p>
                    <img  src="img/4smotenaive.PNG"> <br><br>
                    <p>Performance Matrix on train data set with SMOTE</p>
                    <img  src="img/4smotetrain.PNG"> <br><br>
                    <p>Performance Matrix on test data set with SMOTE</p>
                    <img  src="img/4smotetest.PNG"> <br><br>

                    <div class="subheading mb-3">CROSS VALIDATION</div>
                    <p>To increase the accuracy of the training set. We are using 10 folds for the cross validation</p>
                    <img  src="img/4crosstrain.PNG"> <br><br>
                    <img  src="img/4crosstest.PNG"> <br><br>
                    <p>From the above 10 fold cross validation we can infer that the test score has improved from the initial 76% to 83%</p>
                    <p>I tried to improve the accuracy score by using SMOTE technique and K fold cross validation. from the above experimentation we can see that while using the 10 fold cross validation has improved the accuracy score to 83%</p>

                    <div class="subheading mb-3">REFERENCES</div>
                    <p>Please find below the references I used to understand the concept and build the code</p>
                    <p>1) To understand the indepth <a href="https://stackoverflow.com/questions/10059594/a-simple-explanation-of-naive-bayes-classification/20556654#20556654">concept of Naive Bayes</a></p>
                    <p>2) To generate <a href="https://www.dezyre.com/recipes/generate-classification-report-and-confusion-matrix-in-python">confusion matrix and classification report.</a></p>
                    <p>3) To understand the concept of <a href="https://www.dataminingapps.com/2016/11/what-is-smote-in-an-imbalanced-class-setting-e-g-fraud-detection/">SMOTE technique</a></p>
                    <p>4) Concept of <a href="https://medium.datadriveninvestor.com/k-fold-cross-validation-6b8518070833">K-fold cross validation</a></p>









                  




                
                
                 





























            <h3 class="mb-0">BLOG 4 - Sentiment analysis on IMDB reviews using NBC-Naive Bayes Classifier </h3></br>
            <p>In this blog we will be learning about the concept of Naive bayes where we will be performing Sentiment Analysis on reviews from
              IMDB website.First lets see the basic concept of Naive Bayes.</p>
              <p>The link to my complete code in <a href="https://github.com/nssn96/Sentiment_analysis-NBC/blob/main/NBC_submitted.ipynb">Google colab</a></p>
              <div class="subheading mb-3">NBC-Introduction</div>
              <p>Naive Bayes classifier is classification algorithm based on Bayes Theorem.Bayes’ Theorem finds the probability of an event occurring given the probability of another event that has already occurred. Bayes’ theorem is stated mathematically as the following equation:</p>
              <img  src="img/bayes1.PNG"> <br><br>
              <p>P(A|B)-- Probability of Event A occuring given Event B has already occured</br>
                P(B|A)-- Probability of Event B occuring given Event A has already occured</br>
                P(A)--  Probability of Event A</br>
                P(B)-- Probability of Event B</br></p>
                <p>The fundamental Naive Bayes assumption is that each feature makes an equal and independent contribution to the outcome.In relation
                  to our dataset we assume that no pair of features are dependent and each feature is given the same weight(or importance).</p>
                  <P>Naive Bayes is naive because of its assumption of the conditional independence because it is almost never true in the real world.
                    Naive Bayes classifier can make decisions with partial information about attributes, even in absence of information about any attributes
                    we can use Apriori probabilities of class variable.</P>
                    <div class="subheading mb-3">About Dataset</div>
                    <p>I am importing the dataset directly from the Kaggle webbsite.This dataset was created for the Paper 'From Group to Individual Labels using Deep Features', Kotzias et. al,. KDD 2015
                      It contains sentences labelled with a positive or negative sentiment.
                    <p>Format : sentence  \t  score</br>
                      Details : Score is either 1 (for positive) or 0 (for negative)</p>
                      <p>The IMDB dataset consists of 500 positive and 500 negative sentences. Those were selected randomly for larger datasets of reviews.
                        We attempted to select sentences that have a clearly positive or negative connotaton, the goal was for no neutral sentences to be selected.</p>
                        <div class="subheading mb-3">Reading the data from text file</div>
                        <img  src="img/3import.PNG"> <br><br>
                        <p>Looking at the sample of data on how it looks.We can see that it contains two columns Review and Sentiment. The corresponding reviews have sentiment either 0/1(negative/positive)</p>
                        <img  src="img/3head.PNG"> <br><br>
                        <div class="subheading mb-3">Preprocessing the dataset</div>
                        <p>Before dividing into test and training sets, we need to pre-process the data, so we can use it directly. we will be removing special characters and replacing them with spaces. Pre-processing is very much required as dataset consists of many special characters like /,.'' and
                          numbers. If we dont remove those special characters the vocabulay word count that we will be doing in the subsequent steps would not work properly.</p>
                          <img  src="img/3pre.PNG"> <br><br>
                          <p>The dataset looks like below after the data is pre-processed and removed of all special characters</p>
                          <img  src="img/3afterpre.PNG"> <br><br>
                          <div class="subheading mb-3">Divide the dataset as train, development and test</div>
                          <p>Setting up the training, development (dev) and test sets has a huge impact on productivity or accuracy of the model. It is important to choose sets from the same distribution and it must be taken randomly from the dataset. Best practice is to split it to 80:10:10--> 80% train,10% test and 10% dev</p>
                          <img  src="img/3split.PNG"> <br><br>
                          <div class="subheading mb-3">Build a vocabulary as list from test data</div>
                          <p>For the vocabulary builder , we will be having three values against each word.the first value is the count of occurance of the word in a negative(0) review , the second value is the count of occurance of the word in a Positive(1) review and third value is the total count of that word in all reviews. 
                            We also omit the rare words for which the occurance is less than 5 times(which we will be passing as parameter to the function). We will be using the function vocabulary_build() for the vocabulary builder</p>
                            <img  src="img/3vocabbuild.PNG"> <br><br>
                          <p>The output of the above function will look like below:</p>
                          <img  src="img/3vocabwords.PNG"> <br><br>
                          <div class="subheading mb-3">Calculating the Probability of occurance of the words</div>
                          <p>The function word_probability that we have used takes two argument one is the dataset or the dataframe and a word for which
                            we need to find the probability of occurance in the document.For this the formula we use P(Word)=(num of times the word occures in the document / Total number of words in the documents). </p>
                        <p>So, in this function what we are doing is reading the stentences from the data and splitting each sentences and storing it in an array.If we find the required word in the sentence array then we are incrementing the count by 1. Then finally divide the count by the length of the data(which is the total number of words in the document) will give the probablity of the occurance of the word.</p>
                        <img  src="img/3wordprob.PNG"> <br><br>
                        <p>The below code is for finding the probability of occurance for all the words in the document.</p>
                        <img  src="img/3allprob.PNG"> <br><br>
                        <p>and its few of the output shown below</p>
                        <img  src="img/3allproboutput.PNG"> <br><br>
                        <div class="subheading mb-3">P[“the”] = num of documents containing ‘the’ / num of all documents</div>
                        <img  src="img/3the.PNG"> <br><br>
                        <div class="subheading mb-3">Conditional probability based on the sentiment</div>
                        <p>In this step we will be finding the conditional probablity.The sentiment_probability is a function which is dependent on another function count_all which is used to find the count of all positive and negative sentiment in the given dataset</p>
                        <p>The formula for finding the conditional probablity is
                          P(word|Sentiment)= (P(Sentiment|word)*P(word))//P(Sentiment)</p>
                          <p>The count array contains the count of all positive and negative sentiment in the dataset.If the sentiment passed is 1 then respective count value is given to the count.If the sentiment passed is 0 then respective count value is given to the count.Then applying the above formulla we can get conditional probablity of the desired word for given sentiment.</p>
                          <img  src="img/3cp1.PNG"> <br><br>
                          <img  src="img/3cp2.PNG"> <br><br>
                          <div class="subheading mb-3">P[“the” | Positive]  = # of positive documents containing “the” / num of all positive review documents</div>
                          <img  src="img/3theP.PNG"> <br><br>
                          <div class="subheading mb-3">Calculate accuracy using dev dataset </div>
                          <p>We will be conducting the five fold cross validation</p>
                          <p>K-Fold Cross Validation is a concept where a given data set is divided into a K number of fold or sections where each fold is used as a testing set. In our scenario we will be performing the 5-Fold cross validation(k=5).Here, the data set is split into 5 folds.In the first iteration, the first fold will be used as test model and the rest 4 folds will be used for training. In the second iteration, 2nd fold is used as the testing set while the rest serve as the training set. This process is repeated until each fold of the 5 folds have been used as the testing set.</p>
                          <p>In the Kfold_accuracy function first calculation of the fold size is done by dividing the size of data by the value of K.Thus if the data size is 100 and k is 5 each fold contains 20 sentences.</p>
                          <img  src="img/3ff1.PNG"> <br><br>
                          <img  src="img/3ff2.PNG"> <br><br>
                          <img  src="img/3ff3.PNG"> <br><br>
                          <img  src="img/3ff4.PNG"> <br><br>
                          <p>Some of the output below for the 5 folds validation</p>
                          <img  src="img/3ff5.PNG"> <br><br>
                          <div class="subheading mb-3">Comparing the effect of Smoothing and calculation of the final accuracy</div>
                          <p>The zero probability problem in Naïve Bayes, where the entire expression becomes zero if one of the conditional probabilities is zero,
                            can be handeled by using Laplace smoothing.The smoothing technique that has been used here is the Laplace estimate which uses alpha as the smoothing parameter.</p>
                           <p>If the alpha value is zero then there will not be any smoothing and the probablity will be zero if any of the feature doesnt have a value,but
                            if alpha value is set to 1, then even though any feature value is missing the probability will not become zero.</p> 
                         <p>The cal_accuracy function depends on predict function.The predict function takes 4 arguments the review, vocabulary, label_count, lam(the
                          smoothing parameter).The role of the predict function is to calculate the probability ->  prob += ((vocabulary[word][label]+lam) / (label_count[label]+(lam*len(vocabulary)))), this will find the probablity for both positive sentiment and a negative sentiment.If the negative probablity is greater than the positive probablity then returns 0 else it returns 1.So eventually the predict function predicts the sentiment based on the probability of the word.
                          </p>
                          <p>After applying smoothing on the test_data the final accuracy comes out to be 60%</p>
                          <img  src="img/3fc.PNG"> <br><br>

                          <div class="subheading mb-3">Derive Top 10 words that predicts positive and negative class</div>
                          <img  src="img/3tt1.PNG"> <br><br>
                          <img  src="img/3tt2.PNG"> <br><br>

                          <div class="subheading mb-3">REFERENCES</div>
                          <p>Please find below the references I used to understand the concept and build the code</p>
                          <p>1) To understand the indepth <a href="https://stackoverflow.com/questions/10059594/a-simple-explanation-of-naive-bayes-classification/20556654#20556654">concept of Naive Bayes</a></p>
                          <p>2) Different steps in <a href="https://www.kdnuggets.com/2018/03/text-data-preprocessing-walkthrough-python.html">Pre-processing of text data</a></p>
                          <p>3) To understand the <a href="https://rafalab.github.io/dsbook/smoothing.html">concept of Smoothing</a></p>
                          <p>4) Concept of <a href="https://medium.datadriveninvestor.com/k-fold-cross-validation-6b8518070833">K-fold cross validation</a></p>




























            <h3 class="mb-0">BLOG 3 - Concept of overfitting </h3></br>
            <p>In this blog we will be learning about the concept of overfitting using Higher order linear regression.First lets see 
              the basic definition of overfitting and underfitting.</p>
              <div class="subheading mb-3">overfitting</div>
              <p>Overfitting refers to model that models the training data too well. It will even pick the the noise
                and random fluctuations and lean as concept.When model is too complex, training error is small but test error is large</p>
                <p>when Model Complexity > Available Data overfitting occurs</p>

                <div class="subheading mb-3">underfitting</div>
                <p>Underfitting occurs when the model or algorithm does not fit the data well enough i.e when the model is to simple</p>
                <p>when Model Complexity < Available Data underfitting occurs</p>

                <p>The link to my complete code on <a href="https://github.com/nssn96/overfitting/blob/main/Nadhamuni_Suresh_02.ipynb">Google colab</a></p>

                <p>Lets see the each steps that I used to approach the problem where I reduced the loss and increase the performance of the model</p>
                
                <div class="subheading mb-3">PART A - Generating data points (X, Y) using y = sin(2*pi*X) + N</div>

              <p>In the below code we will be generating 20 sample data points using uniform distribution between 0 and 1 for X.We will sample N from the normal gaussian distribution 
                Then implementing the function y as sin(2.pi.x) .The use of random seed allows us to remove the value of 0 in the normal and uniform distributions.
              </p>
              <img  src="img/a.PNG"> <br><br>
              <img  src="img/a_img.PNG"> <br><br>
              <div class="subheading mb-3">Splitting the data points to train and test set</div>
              <p>In this step we will be splitting the sample data points into equal halves of training data and testing data.We had 
                generated 20 data points in the first step, so will be splitting 10 data points of training and 10 data points of test data.
    
              </p>
              <img  src="img/splitnew.PNG"> <br><br>
              <div class="subheading mb-3">A simple plot of train and test data</div>
              <img  src="img/split_img1.PNG"> <br><br>
              <img  src="img/splitimg2.PNG"> <br><br>
              <div class="subheading mb-3">PART B -	Using root mean square error, find weights of polynomial regression for order is 0, 1, 3, 9</div>
              <p>In this step we will find the weights for the orders 1,3,6,9(Higher order).We will be using the training data in this step.I have used pipeline concept in this step, it basically combines many features into a single model.Since I have used Linear regression the Pipeline will encapsulate the linear regression and degrees into a single variable model.
                We can then train this model to get the weights and get the training and test erros Then from the Model we can fit in the X_train and y_train.
                We can retrive the weights by means of .coef_ .The output values will be predicted using model.predict function.
                The RMS is calculated using the mean_squared_error() function.</p>
              <img  src="img/partb.PNG"> <br><br>
              <div class="subheading mb-3">PART C -	Display weights in table </div>
              <p>In this part we will be displaying the weights in a tabular format</p>
              <img  src="img/partc.PNG"> <br><br>
              <p>From the above table we can see that the magnitude of the coefficients/Weights increases
                dramatically as the order of the polynomial increases.
              </p>
              <div class="subheading mb-3">PART D -	Draw a chart of fit data </div>
              <p>In this part we will be plotting the graph between the predicted value and actual input for the different orders that we have used.</p>
              <div class="subheading mb-3">Order 0 </div>
              <img  src="img/order0.PNG"> <br><br>
              <div class="subheading mb-3">Order 1 </div>
              <img  src="img/order1.PNG"> <br><br>
              <div class="subheading mb-3">Order 3 </div>
              <img  src="img/order3.PNG"> <br><br>
              <div class="subheading mb-3">Order 9 </div>
              <img  src="img/order9.PNG"> <br><br>

            <div class="subheading mb-3">PART E -	Draw train error vs test error </div>
            <p>In this step we have plotted the graph between Training and test error.</p>
            <img  src="img/parte.PNG"> <br><br>
            <p>From the above graph we can clearly see that the test error is very high for the degree 9. Our goal is to reduce that test error for higher
              order functions.</p>

            <div class="subheading mb-3">PART F -Generate 100 more data and fit 9th order model and draw fit </div>
            <img  src="img/partf.PNG"> <br><br>
            <img  src="img/partf_img.PNG"> <br><br>
            <p>This is similar to the first step that we did for the sample data generation, I have just increased the number of sample points. From the graph we can infer that when the number of data points increases then the prediction is almost accurate to the actual data.</p>

            <div class="subheading mb-3">PART G - Draw chart for lambda is 1, 1/10, 1/100, 1/1000, 1/10000, 1/100000</div>
            <p>we have plotted the Samples Actual Value and Predicted value graphs for all the lambda values.The errors are calculated in the previous step</p>

            <img  src="img/parth.PNG"> <br><br>
            <img  src="img/parth_img1.PNG"> <br><br>
            <img  src="img/parth_img2.PNG"> <br><br>

            <div class="subheading mb-3">PART H -	Now draw test  and train error according to lamda </div>
            <img  src="img/parti.PNG"> <br><br>
            <p>From the above plot we can see that the for 0.001 lambda value the test error id very less. So, After experimentation the best model with the low test error is with lambda value of 0.001</p>

            <div class="subheading mb-3">Challenges</div>
            <p>The challenge that I faced while doing this code was, I was not able to implement the Regularization concept even though
              I understood the need of regularization or what it is therotically.
            </p>
   
            <div class="subheading mb-3">References</div>
              <p>Please find below the references I used to understand the concept and build the code.</p>
              <p>1)Understanding <a href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html"> underfitting vs overfitting</a> using scikitlearn</p>
              <p>2) <a href="https://numpy.org/doc/stable/reference/random/generated/numpy.random.uniform.html">Numpy documentation</a>- To 
                understand the concepts of random.uniform and random.normal functions which was used to generate the data(x,y)</p>
                <p>3) Used this <a href="https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c" > blog </a> understand the Regularization Concept</p>
              <p>4) <a href="https://www.w3schools.com/python/python_ml_scatterplot.asp" >w3school webiste</a> to understand the concept of plotting graphs</p>
          
            


              















            <h3 class="mb-0"> BLOG 2 - Training an image classifier using CNN on the CIFAR-10 dataset</h3></br>
            <p>We will be building an image classifier using CNN(convolutional Neural Net) on the famous dataset--The CIFAR-10 dataset. Please find below the complete guide with code and  various steps involved from downloading the dataset to building ,training and testing the model</p>
            <p>The link to my complete code on <a href="https://github.com/nssn96/Image_Classifier/blob/main/Image_classifier_CIFAR10.ipynb">Google colab</a></p>
            <p>Lets see the each steps that I used to approach the problem where I improved my accuracy from 54% to 71% </p>
            <div class="subheading mb-3">Exploring the dataset</div>
            <p>Data exploration is one of the most important step in any machine learning problem. If you understand your data well, we have half the solution since you will be building the model depending on the data. Before starting to work on any dataset, how many classes are there and what the images look like, what is the size of dataset Here, in the CIFAR-10 dataset,</p>
            <p>1)Images are of size 32X32X3 (32X32 pixels and 3 colour channels namely RGB(Red-Green-Blue)</p>
            <p>2)There are 10 classes. (classes = ('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')</p>
            <p>3)The dataset consists of 60000 with 6000 images per class.</p>
            <p>4)There are 50000 training images (this means we get 5000 images per class for training our NN) and 10000 test images.</p>
            <p>We can download the dataset train and test datasets as follows:</p>
            <img  src="img/NN_imageimport.PNG"> <br><br>
            
            <div class="subheading mb-3">Checking the images from the dataset </div>
            <p>The code to check some images from the dataset</p>
            <img  src="img/images.PNG"> <br><br>

            <div class="subheading mb-3">Define a Convolutional Neural Network </div>
            <p>Now, we will create a basic model for solving our classification problem. We will be building using the nn.Module class already provided by PyTorch, it contains the initialization and forward methods.</p>
            <p>For training our model we need to form batches of images from our dataset, so we will be using the DataLoader() function provided by PyTorch. DataLoader gives us a dynamic environment to create the batches and makes our data easily iterable.</p>
             <p>In this code, I have implemented <b>three convolution layers and five linear layers</b></p>
             <img  src="img/CNN_model.PNG"> <br><br>

             <div class="subheading mb-3">Use of GPU(Graphics processing unit) in processing data</div>
             <p>Initially I ran the program on CPU, since the number of networks and layer was very small and the epoch times was just two times. But once I started experimenting by increasing the number of convolution and linear layer and the epoch times, the program started running for more than 10 minutes which was very time consuming. That’s when I got the idea of using GPUs instead of CPUs for training the network, as it will be faster, and the CPU will be free to perform other processes.</p>
             <P>To enable GPU in our Google Colab, we need to select the following option Runtime-->change runtime type.</P>
             <img  src="img/GPU1.PNG"> <br><br>
             <img  src="img/GPU2.PNG"> <br><br>
             <p>For the GPU to work we need to  send the inputs and targets at every step to the GPU too and the model must be in GPU as well.We need to make the below changes in the code for the GPU to work</p>
             <img  src="img/GPU3.PNG"> <br><br>
             <img  src="img/GPU4.PNG"> <br><br>

             <p>After doing this you will notice a massive speed up in your training of network.</p>

             <div class="subheading mb-3">Define a Loss function and optimizer</div>
             <p>The loss function used is the cross-entropy loss which has in-built negative log likelihood loss and softmax also, which can be easily used for the classification task.
              The optimizer was initially used with SGD(Stochastic gradient descent) but I am experimenting with various other optimizers like Adam(Adaptive Moment Estimation), Adagrad,Rprop,RMSprop and fine tuning it to improve the accuracy.
              </p>
              <img  src="img/optim.PNG"> <br><br>

              <div class="subheading mb-3">Train the network </div>
              <p>In this step we just need to loop over our data iterator, and feed the inputs to the network and optimize.Finally we save the trained model.</p>
              <img  src="img/CNN_train.PNG"> <br><br>

              <div class="subheading mb-3">Test the network on the test data</div>
              <p>This is the final step, we have built our model, trained the model, this step is to test our model and how accurate it is on our prediction.</p>
              <p>The below output will show the average accuracy of the network and the accuracy of each classes.</p>
              <img  src="img/Test1.PNG"> <br><br>
              <img  src="img/Test2.PNG"> <br><br>
              <p>The Average accuracy of the Network</p>
              <img  src="img/acc.PNG"> <br><br>
              <p>The Average of each classes</p>
              <img  src="img/acc2.PNG"> <br><br>


              <div class="subheading mb-3">Some of the challenges/Observations during this Model Development</div>
              <p>•Using GPU will save a lot of training time when compared to CPU, when you increase the Convolutional, linear layers layer and the epoch times.</p>
              <p>•	When I used SGD as optimizer, it was very slow as it oscillates when there are deep sides. But using Adam (Adaptive Moment Estimation) the learning was better and fast, it took a smaller number of epochs to train the network, its adaptive learning rate, bias-correction and momentum make it a good choice.</p>
              <p>•	You can change the number of epochs, more epochs mean more training and better the model in its accuracy.</p>


            <div class="subheading mb-3">Different efficiencies with changes in the epoch,cov layers and optimizers</div>
            <p>Please find below the different trials that I did using different models,changing the layers and the parameters</p>
            <img  src="img/eff.PNG"> <br><br>

              <div class="subheading mb-3">References</div>
              <p>Please find below the references I used to understand the concept and build this model</p>
              <p>1) <a href="https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py">Pytorch tutorial for Image classifier</a></p>
              <p>In this tutorial a basic Neural Net(2x3  2-convolution,3-Linear) was used and SGD was used as an optimizer. I have experimented by increasing the convolutional and linear layes,using a Adam optimizer,increasing the epoch times and running on GPU for time efficiency. This resulted in improving my accuracy to 71%</p>
              <p>2)<a href="https://colab.research.google.com/drive/1anXOqz-artoBWbdK0Ff2IYDtwe5PpRqH?usp=sharing">My Google colab notebook</a></p>
              <p>3)<a href="https://www.cs.toronto.edu/~kriz/cifar.html">The CIFAR-10 dataset</a>-for exploratory data analysis</p>
              <p>4)<a href="https://becominghuman.ai/build-your-own-neural-network-for-cifar-10-using-pytorch-9bdffb389b7a">Build your own Neural Net</a></p>


              












            <h3 class="mb-0">BLOG 1 - Titanic - Machine Learning from Disaster Project from Kaggle Walk Through</h3></br>
            <p>In this blog, I hope to show how I worked through the famous Titanic problem in kaggle for Beginners. 
              The goal is to correctly predict if someone survived the Titanic shipwreck.</p>
            <p> In this project we need to create a machine learning model that predicts which passengers survived the Titanic shipwreck.
               In this challenge, I built a predictive model that answers the question: “what sorts of people were more likely to survive?” using passenger data (ie name, age, gender, socio-economic class, etc). 
              </p>
              <p> Lets see the each steps that I used to approach the problem where I improved my score of  
                <b>0.76555</b> to <b>0.77511</b>. I am adding my <a href="https://www.kaggle.com/nssurya1996/titanic-problem"> kaggle Notebook </a>that I used for the Project for reference</p> </br>
                <div class="subheading mb-3">DATASET </div>
                <p>There are two files that we would be given under the data tab on the top of the competition page. 
                  The two files that we use are the : (1) train.csv (2) test.csv</p>
                  <p><b>train.csv</b> contains the details of a subset of the passengers on board (891 passengers, where each passenger gets a different row in the table).
                    The values in the second column "Survived" can be used to determine whether each passenger survived or not. If it is '1' - the passenger survived, if '0' the passenger died.
                  </p>
                  <p><b>test.csv</b>-- Using the patterns we find in train.csv, we have to predict whether the other 418 passengers on board (in test.csv) survived.</p>
                  <p>Our prediction submission file must contain only two columns, if we have more than that we will face issue while submission.The file must contain a "PassengerId" column containing the IDs 
                    of each passenger from test.csv.A "Survived" column (that we will create!) with a "1" for the rows where we think the passenger survived, and a "0" where we predict that the passenger died.</p>
                    <div class="subheading mb-3">The different steps I used in this Project </div>
                    <div class="subheading mb-3">1) Importing the different libraries used in the code </div>
                    <img  src="img/import.png"> <br><br>
                    <div class="subheading mb-3">2) Loading the training data to variable(train_data) and analysing it</div>
                    <img  src="img/training.png"> <br><br>
                    <img  src="img/traininfo.png"> <br><br>
                    <p>From the above table we can see that there are lot of null values in Age and Cabin Column and two null values in Embarked Column out of total 891 entries</p>
                    <img  src="img/trainhead.PNG"> <br><br>
                    <p>To better understand the numerical data , we can use the .describe() method.This gives us an understanding of the central
                      tendencies of the data</p>
                      <img  src="img/describe.PNG"> <br><br>
                    <div class="subheading mb-3">3) Loading the test data to variable(test_data)  and analysing it</div>
                    <img  src="img/test.PNG"> <br><br>
                    <div class="subheading mb-3">4) Splitting the numerical and categorical data</div>
                    <img  src="img/splitdata.PNG"> <br><br>
                    <div class="subheading mb-3">5) Plotting a histogram for all the numerical data</div>
                    <img  src="img/hist.PNG"> <br><br>
                    <img  src="img/hist1.PNG"> <br><br>
                    <img  src="img/hist2.PNG"> <br><br>
                    <p>From the above histogram we can know that the Age is a normal distribution whereas the other features are not normally distributed. So, we might need to normalise the features like Fare, to have its correct contribution during the Prediction</p>
                    <div class="subheading mb-3">6) Corrplot</div>
                    <img  src="img/corr.PNG"> <br><br>
                    <img  src="img/corr1.PNG"> <br><br>
                    <p>From the above heatmap we can infer some correlations between features. for eg: 1) The number of parents and number of sibilings ie family tend to travel together.2)The Age and the number of siblings(SibSp) has a negative correlation and such. These help us to understand the different relationship in our data.</p>
                    <div class="subheading mb-3">7) Pivot table-Comparing survival rate across numeric variables</div>
                    <img  src="img/pivot-num.PNG"> <br><br>
                    <p>Even though it might not contribute directly, we might get a understanding of the data like younger people have survived more and the people who paid high fare had high chances of survival. These are the things we need to make note of while building our models.</p>
                    <div class="subheading mb-3">8) Categorical Data--Bar charts to understand the categorical data</div>
                    <img  src="img/cat.PNG"> <br><br>
                    <img  src="img/cat1.PNG"> <br><br>
                    <img  src="img/cat2.PNG"> <br><br>
                    <img  src="img/cat3.PNG"> <br><br>
                    <img  src="img/cat4.PNG"> <br><br>
                    <img  src="img/cat5.PNG"> <br><br>
                    <img  src="img/cat6.PNG"> <br><br>
                    <div class="subheading mb-3">9) Comparing survival and each of these categorical variables</div>
                    <img  src="img/catsurvival.PNG"> <br><br>
                    <img  src="img/catsurvival1.PNG"> <br><br>
                    <p>From the above pivot table we can understand that relatively the lot of people from first class survived,lot of women compared to men were rescued first.We can understand that this feature(sex) will play an important contribution in the prediction.</p>
                    <div class="subheading mb-3">10) To find the percentage of the women survived in titanic</div>
                    <img  src="img/womensurvival.PNG"> <br><br>
                    <div class="subheading mb-3">11) To find the percentage of the Men survived in titanic</div>
                    <img  src="img/mensurvival.PNG"> <br><br>
                    <div class="subheading mb-3">12) Developing a model-RandomeForestClassifer model</div>
                    <p>I have used what's known as a random forest model. This model is constructed of several "trees" that will individually consider each passenger's data and vote on whether the individual survived. Then, the random forest model makes a decision based on majority: the outcome with the most votes wins.The code cell below looks for patterns in four different columns ("Pclass", "Sex", "SibSp", and "Parch") of the data. It constructs the trees in the random forest model based on patterns in the train.csv file, before generating predictions for the passengers in test.csv.</p>
                    <p>Creating the RandomeForestClassifer model to train and fit the data. After that we predict whether the passengers in the test_data had survived or not(0/1). At the end we load the predictions to prediction_submission.csv.</p>
                    <img  src="img/model.PNG"> <br><br>
                      






                    
                    


             
        

            
            
             
          </ul>
        </div>
      </section>









    </div>

    <!-- Bootstrap core JavaScript -->
    <script src="vendor/jquery/jquery.min.js"></script>
    <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="vendor/jquery-easing/jquery.easing.min.js"></script>

    <!-- Custom scripts for this template -->
    <script src="js/resume.min.js"></script>

  </body>

</html>
