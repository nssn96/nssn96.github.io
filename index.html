<!DOCTYPE html>
<html lang="en">

  <head>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Surya's portfolio</title>

    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom fonts for this template -->
    <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:100,200,300,400,500,600,700,800,900" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i,800,800i" rel="stylesheet">
    <link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet">
    <link href="vendor/devicons/css/devicons.min.css" rel="stylesheet">
    <link href="vendor/simple-line-icons/css/simple-line-icons.css" rel="stylesheet">

    

    <!-- Custom styles for this template -->
    <link href="css/resume.min.css" rel="stylesheet">

  </head>

  <body id="page-top">

    <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
      <a class="navbar-brand js-scroll-trigger" href="#page-top">
        <span class="d-block d-lg-none">Start Bootstrap</span>
        <span class="d-none d-lg-block">
          <img class="img-fluid img-profile rounded-circle mx-auto mb-2" src="img/Profilepic.jpg" alt="">
        </span>
      </a>
      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse" id="navbarSupportedContent">
        <ul class="navbar-nav">
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#about">About</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#experience">Experience</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#education">Education</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#skills">Skills</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#projects">Projects</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#awards">Achievements & Training</a>
          </li>

          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#blogs">Blogs</a>
          </li>
    
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="https://drive.google.com/file/d/1Fi63j5H90e000iNTPpHUmKUrJVbUeKXE/view?usp=sharing">Resume</a>
          </li>


        </ul>
      </div>
    </nav>

    <div class="container-fluid p-0">

      <section class="resume-section p-3 p-lg-5 d-flex d-column" id="about">
        <div class="my-auto">
          <h1 class="mb-0">Surya
            <span class="text-primary">Narayanan N S</span>
          </h1>

         

          <!-- Header Headlines -->
          <p class='hero-subtitle'>
            I am a <span class='typist-text typist-blink' data-typist='ML Enthusiast,Amateur Photographer' id='typist-element'>
              Code Lover, AI Enthusiast, Electronics Engineer</span>
          </p>

          <div class="subheading mb-5">Arlington,TX--(214) 929-0233--
            <a href="mailto:name@email.com">nssurya1996@gmail.com</a>
          </div>
          <p class="mb-5">I am a Software Developer with experience in SOA architecture and JAVA with an interest to learn and master Software Development, Robotics and AI.
             Looking for an opportunity where I can use my knowledge and understanding from my EIE undergrad to focus on the application of AI methodologies for solving real-world problems in the electronics field.</p>
          <ul class="list-inline list-social-icons mb-0">
           <!---- <li class="list-inline-item">
              <a href="https://www.facebook.com/profile.php?id=100006363160192">
                <span class="fa-stack fa-lg">
                  <i class="fa fa-circle fa-stack-2x"></i>
                  <i class="fa fa-facebook fa-stack-1x fa-inverse"></i>
                </span>
              </a>
            </li> -->
            
            <li class="list-inline-item">
              <a href="https://www.linkedin.com/in/surya-narayanan-nadhamuni-suresh-bb1686124?lipi=urn%3Ali%3Apage%3Ad_flagship3_profile_view_base_contact_details%3BsLNXNBS9QbKaid4vslazGQ%3D%3D">
                <span class="fa-stack fa-lg">
                  <i class="fa fa-circle fa-stack-2x"></i>
                  <i class="fa fa-linkedin fa-stack-1x fa-inverse"></i>
                </span>
              </a>
            </li>
            <li class="list-inline-item">
              <a href="https://github.com/nssn96">
                <span class="fa-stack fa-lg">
                  <i class="fa fa-circle fa-stack-2x"></i>
                  <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                </span>
              </a>
            </li>
          </ul>
        </div>
      </section>

      <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="experience">
        <div class="my-auto">
          <h2 class="mb-5">Experience</h2>

          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Programmer Analyst</h3>
              <div class="subheading mb-3">Cognizant Technology Solutions India</div>
              <div class="subheading mb-3">Project Name</div>
              <p>Schneider National, US -- L2 Support and Enhancements Project for Schneider Fleet management System (Integration and Telematics (IoT Solutions))</p> 
              <div class="subheading mb-3">Project Description </div>
                <p>
                Schneider Asset and Non-Asset IT Landscape comprises various Oracle Solutions (OTM, B2B, BI, EBS, HCM) and these oracle applications are integrated through various Oracle Fusion Middleware products such as OSB, SOA, ODI, ADF. Schneider Next-generation Fleet management System IT landscape is built on AWS which is being utilized to develop IoT solutions. IoT Solutions which are built to provide seamless business are Platform Science, Orion, Kepler, Kabana and other ELK stack solutions 
                </p>
                <div class="subheading mb-3">Role & Responsibilities</div>
                <p>• Worked extensively in ODI/SOA/OSB Migration From 11g to 12c.</br>
                  <br>•	Supporting critical production activities and monitoring.</br>
                  <br>•	Provide a real-time solution to the issues faced in the Telematics region by interacting with the drivers and going through the IoT platforms (Orion, Platform science, Kibana, Kepler).</br>   
                  <br>•	Monitor business transactions end to end in FMW consisting of OSB, SOA, ODI.</br>
                  <br> •	Performing load testing and analysis of performance in lower MW environments</br> 
                  <br>•	Understanding of business impacts, SLA </br>
                  <br>•	Log/RCA analysis in case of production issues</br> 
                  <br>•	OSB/ODI enhancements according to the new business requirement or as a part of bug fixing.</br> 
                  <br>•	Responsible for automation of job monitoring reports sent as a part of the production support activity.</br> 
                  </p>
            </div>
            <div class="resume-date text-md-right">
              <span class="text-primary">November 2017-December 2020</span>
            </div>
          </div>

      

        </div>

      </section>

      <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="education">
        <div class="my-auto">
          <h2 class="mb-5">Education</h2>

          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">UNIVERSITY OF TEXAS AT ARLINGTON</h3>
              <div class="subheading mb-3">Master of Science in Computer Science-Pursuing</div>
              <p>Relevant Coursework:</br>
                •	Design & Analysis of Algorithms</br>
                •	Data Mining</br>
                •	Web Data management </br>
                </p>
            </div>
            <div class="resume-date text-md-right">
              <span class="text-primary">Jan 2021 - May 2023</span>
            </div>
          </div>

          <div class="resume-item d-flex flex-column flex-md-row">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Amrita School of Engineering, Bengaluru</h3>
              <div class="subheading mb-3">Bachelor of Technology in Electronics and Instrumentation Engineering</div>
              <p>GPA: 7.66</p>
              <p>
                Relevant Coursework</br>
                •	Digital Systems</br>
                •	Digital Signal Processing</br>
                •	Embedded Systems</br>
                •	Robotic Control</br>
                •	Pattern Recognition Techniques and Algorithms</br>
                •	IT Essentials</br>
                •	Econometrics</br>
                </p>
            </div>
            <div class="resume-date text-md-right">
              <span class="text-primary">August 2013 - May 2017</span>
            </div>
          </div>

        </div>
      </section>

      <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="skills">
        <div class="my-auto">
          <h2 class="mb-5">Skills</h2>

          <div class="subheading mb-3">Programming Languages</div>
          <p>C, C++, Java , XML , SpringBoot , Python(basic)</p>

          <div class="subheading mb-3">Software tools</div>
          <p>Arduino , MATLAB , LABVIEW</p>

          <div class="subheading mb-3">Architecture</div>
          <p>SOA(11g/12c) , Microservices , MVC</p>

          <div class="subheading mb-3">Database </div>
          <p>MySQL , PL SQL</p>

          
          
          <!--<ul class="list-inline list-icons">
            <li class="list-inline-item">
              <i class="devicons devicons-html5"></i>
            </li>
            <li class="list-inline-item">
              <i class="devicons devicons-css3"></i>
            </li>
            <li class="list-inline-item">
              <i class="devicons devicons-javascript"></i>
            </li>
            <li class="list-inline-item">
              <i class="devicons devicons-jquery"></i>
            </li>
            <li class="list-inline-item">
              <i class="devicons devicons-sass"></i>
            </li>
            <li class="list-inline-item">
              <i class="devicons devicons-less"></i>
            </li>
            <li class="list-inline-item">
              <i class="devicons devicons-bootstrap"></i>
            </li>
            <li class="list-inline-item">
              <i class="devicons devicons-wordpress"></i>
            </li>
            <li class="list-inline-item">
              <i class="devicons devicons-grunt"></i>
            </li>
            <li class="list-inline-item">
              <i class="devicons devicons-gulp"></i>
            </li>
            <li class="list-inline-item">
              <i class="devicons devicons-npm"></i>
            </li>
          </ul> -->

          
            
            
          </ul>
        </div>
      </section>

      <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="projects">
        <div class="my-auto">
          <h2 class="mb-5">Relevant Projects</h2>

          <div class="subheading mb-3">Library Management System </div>
          <p>Programming Language : C++</p>
          <p>In this Library Management System project, you can enter the record of new books and retrieve the details of books available in the library. You can issue the books to the students and maintain their records. Late fine is charged for students who returns the issued books after the due date.Only one book is issued to students. New book is not issued to students those not returned the last book.</p>
         
          <div class="subheading mb-3">GARAGE OPEN-CLOSE SYSTEM </div>
          <p>Software : LABVIEW</p>
          <p>We operate the garage door using a passcode. The username and password are entered, it is compared with the authentic one in the code, and if the input is correct, the garage door automatically opens. If any input data is wrong, the access is denied. I learned from this application that there is extensive integration between computer programming and electronics, where we can use the LABVIEW software for many applications such as automating any process, proof of concept before building the electronic circuits, developing R&D service and testing tools for hardware products.
          </p>

          <a href="https://ieeexplore.ieee.org/document/8091338/metrics#metrics" <div class="subheading mb-3">A study of dealing serially correlated data in Gross Error Detection (GED) techniques</div></a>
          <p>Software : MATLAB</p>
          <p>Data validation and reconciliation (DVR) plays key role in industries because it uses process information and statistical methods to estimate correct measurements from the observed data. DR is very effective when measurement is free from gross error. Data observed from chemical processes can be serially correlated. Serial correlation in data can arise when a process, takes time to adjust, is exposed to prolonged influences, when data is manipulated or smoothened. If serial correlation is not taken care of, then gross error detection may be inaccurate. Hence dealing serial correlation is important in gross error detection (GED) techniques. Most of the techniques implemented in gross error detection require no correlation in the measurement. But experimental data may have serial correlation. In this project, various approaches like variance correction and pre-whitening are implemented to deal serial correlation on ARMA process and measurement test (MT) is applied to detect gross error. Results portray that MT is not an efficient method for GED and amongst the above mentioned two methods, pre-whitening with low variance is better than variance correction method.</p>
        
        
        
        
        
        </div>
      </section>

      <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="awards">
        <div class="my-auto">
          <h2 class="mb-5">Achievements &amp; Training</h2>
          <ul class="fa-ul mb-0">
            <div class="subheading mb-3">Achievements </div>
            <p>• Published a paper in IEEE International Conference on Signal Processing, Informatics, Communication and Energy Systems (SPICES), 2017 on the topic "A study of dealing serially correlated data in Gross Error Detection (GED) techniques"</br>
              •	Participated in the International Collegiate Programming Contest (ICPC), conducted by ACM in 2015.</br>
              
              •	Ranked fourth in the National level Robotics competition - ‘RACE 2014’ in the event “Line Follower” held at Amrita School of Engineering, Bangalore</br>
              •	Participated in LIFO (line follower) and Robowars in RASE’14 conducted by ACROM, Amrita School of Engineering, Bangalore.</br>
              
              •	 Participated Roboslam and LIFO (line follower) in SENTIENCE’2013 conducted by ACROM, Amrita School of Engineering, Bangalore.</br>
              
              •	Secured first place in our inter-department Volleyball tournament in AMRITA college.</br>	
              
              •	A volunteer in OUTREACH social welfare programs in COGNIZANT</br>  </p>

              <div class="subheading mb-3">Training </div>
              <p>• Attended the BASIC ROBOTICS HANDS-ON WORKSHOP conducted by ACROM, Amrita School of Engineering, Bangalore during the academic year 2013.</br>
                •	Participated in VOIP CONTROLLED ROBOTICS workshop conducted by i3indya technologies at ASE, Bangalore on 22nd and 23rd February of 2014.</br>
                •	Attended industrial automation training on Horner PLC conducted by AGIIT.</br>
                
                </p>
            
             
          </ul>
        </div>
      </section>

      <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="blogs">
        <div class="my-auto">
          <h2 class="mb-5">BLOGS</h2>
          <ul class="fa-ul mb-0">

            <h3 class="mb-0">Concept of overfitting </h3></br>
            <p>In this blog we will be learning about the concept of overfitting using Higher order linear regression.First lets see 
              the basic definition of overfitting and underfitting.</p>
              <div class="subheading mb-3">overfitting</div>
              <p>Overfitting refers to model that models the training data too well. It will even pick the the noise
                and random fluctuations and lean as concept.When model is too complex, training error is small but test error is large</p>
                <p>when Model Complexity > Available Data overfitting occurs</p>

                <div class="subheading mb-3">underfitting</div>
                <p>Underfitting occurs when the model or algorithm does not fit the data well enough i.e when the model is to simple</p>
                <p>when Model Complexity < Available Data underfitting occurs</p>

                <p>The link to my complete code on <a href="">Google colab</a></p>

                <p>Lets see the each steps that I used to approach the problem where I reduced the loss and increase the performance of the model</p>
                
                <div class="subheading mb-3">PART A - Generating data points (X, Y) using y = sin(2*pi*X) + N</div>

              <p>In the below code we will be generating 20 sample data points using uniform distribution between 0 and 1 for X.We will sample N from the normal gaussian distribution 
                Then implementing the function y as sin(2.pi.x) .The use of random seed allows us to omit the value of 0 in the uniform and normal distributions.
              </p>
              <img  src="img/a.PNG"> <br><br>
              <img  src="img/a_img.PNG"> <br><br>
              <div class="subheading mb-3">Splitting the data points to train and test set</div>
              <img  src="img/splitnew.PNG"> <br><br>
              <div class="subheading mb-3">A simple plot of train and test data</div>
              <img  src="img/split_img1.PNG"> <br><br>
              <img  src="img/splitimg2.PNG"> <br><br>
              <div class="subheading mb-3">PART B -	Using root mean square error, find weights of polynomial regression for order is 0, 1, 3, 9</div>
              <img  src="img/partb.PNG"> <br><br>
              <div class="subheading mb-3">PART C -	Display weights in table </div>
              <img  src="img/partc.PNG"> <br><br>
              <p>From the above table we can see that the magnitude of the coefficients/Weights increases
                dramatically as the order of the polynomial increases.
              </p>
              <div class="subheading mb-3">PART D -	Draw a chart of fit data </div>
              <div class="subheading mb-3">Order 0 </div>
              <img  src="img/order0.PNG"> <br><br>
              <div class="subheading mb-3">Order 1 </div>
              <img  src="img/order1.PNG"> <br><br>
              <div class="subheading mb-3">Order 3 </div>
              <img  src="img/order3.PNG"> <br><br>
              <div class="subheading mb-3">Order 9 </div>
              <img  src="img/order9.PNG"> <br><br>

            <div class="subheading mb-3">PART E -	Draw train error vs test error </div>
            <img  src="img/parte.PNG"> <br><br>

            <div class="subheading mb-3">PART F -Generate 100 more data and fit 9th order model and draw fit </div>
            <img  src="img/partf.PNG"> <br><br>
            <img  src="img/partf_img.PNG"> <br><br>

            <div class="subheading mb-3">PART G - Draw chart for lambda is 1, 1/10, 1/100, 1/1000, 1/10000, 1/100000</div>

            <img  src="img/parth.PNG"> <br><br>
            <img  src="img/parth_img1.PNG"> <br><br>
            <img  src="img/parth_img2.PNG"> <br><br>

            <div class="subheading mb-3">PART H -	Now draw test  and train error according to lamda </div>
            <img  src="img/parti.PNG"> <br><br>

            <div class="subheading mb-3">Challenges</div>
            <p>The challenge that I faced while doing this code was, I was not able to implement the Regularization concept even though
              I understood the need of regularization or what it is therotically.
            </p>
   
            <div class="subheading mb-3">References</div>
              <p>Please find below the references I used to understand the concept and build the code.</p>
              <p>1)Understanding <a href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html"> underfitting vs overfitting</a> using scikitlearn</p>
              <p>2) <a href="https://numpy.org/doc/stable/reference/random/generated/numpy.random.uniform.html">Numpy documentation</a>- To 
                understand the concepts of random.uniform and random.normal functions which was used to generate the data(x,y)</p>
                <p>3) Used this <a href="https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c" > blog </a> understand the Regularization Concept</p>
              <p>4) <a href="https://www.w3schools.com/python/python_ml_scatterplot.asp" >w3school webiste</a> to understand the concept of plotting graphs</p>
          
            


              















            <h3 class="mb-0">Training an image classifier using CNN on the CIFAR-10 dataset</h3></br>
            <p>We will be building an image classifier using CNN(convolutional Neural Net) on the famous dataset--The CIFAR-10 dataset. Please find below the complete guide with code and  various steps involved from downloading the dataset to building ,training and testing the model</p>
            <p>The link to my complete code on <a href="https://github.com/nssn96/Image_Classifier/blob/main/Image_classifier_CIFAR10.ipynb">Google colab</a></p>
            <p>Lets see the each steps that I used to approach the problem where I improved my accuracy from 54% to 71% </p>
            <div class="subheading mb-3">Exploring the dataset</div>
            <p>Data exploration is one of the most important step in any machine learning problem. If you understand your data well, we have half the solution since you will be building the model depending on the data. Before starting to work on any dataset, how many classes are there and what the images look like, what is the size of dataset Here, in the CIFAR-10 dataset,</p>
            <p>1)Images are of size 32X32X3 (32X32 pixels and 3 colour channels namely RGB(Red-Green-Blue)</p>
            <p>2)There are 10 classes. (classes = ('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')</p>
            <p>3)The dataset consists of 60000 with 6000 images per class.</p>
            <p>4)There are 50000 training images (this means we get 5000 images per class for training our NN) and 10000 test images.</p>
            <p>We can download the dataset train and test datasets as follows:</p>
            <img  src="img/NN_imageimport.PNG"> <br><br>
            
            <div class="subheading mb-3">Checking the images from the dataset </div>
            <p>The code to check some images from the dataset</p>
            <img  src="img/images.PNG"> <br><br>

            <div class="subheading mb-3">Define a Convolutional Neural Network </div>
            <p>Now, we will create a basic model for solving our classification problem. We will be building using the nn.Module class already provided by PyTorch, it contains the initialization and forward methods.</p>
            <p>For training our model we need to form batches of images from our dataset, so we will be using the DataLoader() function provided by PyTorch. DataLoader gives us a dynamic environment to create the batches and makes our data easily iterable.</p>
             <p>In this code, I have implemented <b>three convolution layers and five linear layers</b></p>
             <img  src="img/CNN_model.PNG"> <br><br>

             <div class="subheading mb-3">Use of GPU(Graphics processing unit) in processing data</div>
             <p>Initially I ran the program on CPU, since the number of networks and layer was very small and the epoch times was just two times. But once I started experimenting by increasing the number of convolution and linear layer and the epoch times, the program started running for more than 10 minutes which was very time consuming. That’s when I got the idea of using GPUs instead of CPUs for training the network, as it will be faster, and the CPU will be free to perform other processes.</p>
             <P>To enable GPU in our Google Colab, we need to select the following option Runtime-->change runtime type.</P>
             <img  src="img/GPU1.PNG"> <br><br>
             <img  src="img/GPU2.PNG"> <br><br>
             <p>For the GPU to work we need to  send the inputs and targets at every step to the GPU too and the model must be in GPU as well.We need to make the below changes in the code for the GPU to work</p>
             <img  src="img/GPU3.PNG"> <br><br>
             <img  src="img/GPU4.PNG"> <br><br>

             <p>After doing this you will notice a massive speed up in your training of network.</p>

             <div class="subheading mb-3">Define a Loss function and optimizer</div>
             <p>The loss function used is the cross-entropy loss which has in-built negative log likelihood loss and softmax also, which can be easily used for the classification task.
              The optimizer was initially used with SGD(Stochastic gradient descent) but I am experimenting with various other optimizers like Adam(Adaptive Moment Estimation), Adagrad,Rprop,RMSprop and fine tuning it to improve the accuracy.
              </p>
              <img  src="img/optim.PNG"> <br><br>

              <div class="subheading mb-3">Train the network </div>
              <p>In this step we just need to loop over our data iterator, and feed the inputs to the network and optimize.Finally we save the trained model.</p>
              <img  src="img/CNN_train.PNG"> <br><br>

              <div class="subheading mb-3">Test the network on the test data</div>
              <p>This is the final step, we have built our model, trained the model, this step is to test our model and how accurate it is on our prediction.</p>
              <p>The below output will show the average accuracy of the network and the accuracy of each classes.</p>
              <img  src="img/Test1.PNG"> <br><br>
              <img  src="img/Test2.PNG"> <br><br>
              <p>The Average accuracy of the Network</p>
              <img  src="img/acc.PNG"> <br><br>
              <p>The Average of each classes</p>
              <img  src="img/acc2.PNG"> <br><br>


              <div class="subheading mb-3">Some of the challenges/Observations during this Model Development</div>
              <p>•Using GPU will save a lot of training time when compared to CPU, when you increase the Convolutional, linear layers layer and the epoch times.</p>
              <p>•	When I used SGD as optimizer, it was very slow as it oscillates when there are deep sides. But using Adam (Adaptive Moment Estimation) the learning was better and fast, it took a smaller number of epochs to train the network, its adaptive learning rate, bias-correction and momentum make it a good choice.</p>
              <p>•	You can change the number of epochs, more epochs mean more training and better the model in its accuracy.</p>


            <div class="subheading mb-3">Different efficiencies with changes in the epoch,cov layers and optimizers</div>
            <p>Please find below the different trials that I did using different models,changing the layers and the parameters</p>
            <img  src="img/eff.PNG"> <br><br>

              <div class="subheading mb-3">References</div>
              <p>Please find below the references I used to understand the concept and build this model</p>
              <p>1) <a href="https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py">Pytorch tutorial for Image classifier</a></p>
              <p>In this tutorial a basic Neural Net(2x3  2-convolution,3-Linear) was used and SGD was used as an optimizer. I have experimented by increasing the convolutional and linear layes,using a Adam optimizer,increasing the epoch times and running on GPU for time efficiency. This resulted in improving my accuracy to 71%</p>
              <p>2)<a href="https://colab.research.google.com/drive/1anXOqz-artoBWbdK0Ff2IYDtwe5PpRqH?usp=sharing">My Google colab notebook</a></p>
              <p>3)<a href="https://www.cs.toronto.edu/~kriz/cifar.html">The CIFAR-10 dataset</a>-for exploratory data analysis</p>
              <p>4)<a href="https://becominghuman.ai/build-your-own-neural-network-for-cifar-10-using-pytorch-9bdffb389b7a">Build your own Neural Net</a></p>


              












            <h3 class="mb-0">Titanic - Machine Learning from Disaster Project from Kaggle Walk Through</h3></br>
            <p>In this blog, I hope to show how I worked through the famous Titanic problem in kaggle for Beginners. 
              The goal is to correctly predict if someone survived the Titanic shipwreck.</p>
            <p> In this project we need to create a machine learning model that predicts which passengers survived the Titanic shipwreck.
               In this challenge, I built a predictive model that answers the question: “what sorts of people were more likely to survive?” using passenger data (ie name, age, gender, socio-economic class, etc). 
              </p>
              <p> Lets see the each steps that I used to approach the problem where I improved my score of  
                <b>0.76555</b> to <b>0.77511</b>. I am adding my <a href="https://www.kaggle.com/nssurya1996/titanic-problem"> kaggle Notebook </a>that I used for the Project for reference</p> </br>
                <div class="subheading mb-3">DATASET </div>
                <p>There are two files that we would be given under the data tab on the top of the competition page. 
                  The two files that we use are the : (1) train.csv (2) test.csv</p>
                  <p><b>train.csv</b> contains the details of a subset of the passengers on board (891 passengers, where each passenger gets a different row in the table).
                    The values in the second column "Survived" can be used to determine whether each passenger survived or not. If it is '1' - the passenger survived, if '0' the passenger died.
                  </p>
                  <p><b>test.csv</b>-- Using the patterns we find in train.csv, we have to predict whether the other 418 passengers on board (in test.csv) survived.</p>
                  <p>Our prediction submission file must contain only two columns, if we have more than that we will face issue while submission.The file must contain a "PassengerId" column containing the IDs 
                    of each passenger from test.csv.A "Survived" column (that we will create!) with a "1" for the rows where we think the passenger survived, and a "0" where we predict that the passenger died.</p>
                    <div class="subheading mb-3">The different steps I used in this Project </div>
                    <div class="subheading mb-3">1) Importing the different libraries used in the code </div>
                    <img  src="img/import.png"> <br><br>
                    <div class="subheading mb-3">2) Loading the training data to variable(train_data) and analysing it</div>
                    <img  src="img/training.png"> <br><br>
                    <img  src="img/traininfo.png"> <br><br>
                    <p>From the above table we can see that there are lot of null values in Age and Cabin Column and two null values in Embarked Column out of total 891 entries</p>
                    <img  src="img/trainhead.PNG"> <br><br>
                    <p>To better understand the numerical data , we can use the .describe() method.This gives us an understanding of the central
                      tendencies of the data</p>
                      <img  src="img/describe.PNG"> <br><br>
                    <div class="subheading mb-3">3) Loading the test data to variable(test_data)  and analysing it</div>
                    <img  src="img/test.PNG"> <br><br>
                    <div class="subheading mb-3">4) Splitting the numerical and categorical data</div>
                    <img  src="img/splitdata.PNG"> <br><br>
                    <div class="subheading mb-3">5) Plotting a histogram for all the numerical data</div>
                    <img  src="img/hist.PNG"> <br><br>
                    <img  src="img/hist1.PNG"> <br><br>
                    <img  src="img/hist2.PNG"> <br><br>
                    <p>From the above histogram we can know that the Age is a normal distribution whereas the other features are not normally distributed. So, we might need to normalise the features like Fare, to have its correct contribution during the Prediction</p>
                    <div class="subheading mb-3">6) Corrplot</div>
                    <img  src="img/corr.PNG"> <br><br>
                    <img  src="img/corr1.PNG"> <br><br>
                    <p>From the above heatmap we can infer some correlations between features. for eg: 1) The number of parents and number of sibilings ie family tend to travel together.2)The Age and the number of siblings(SibSp) has a negative correlation and such. These help us to understand the different relationship in our data.</p>
                    <div class="subheading mb-3">7) Pivot table-Comparing survival rate across numeric variables</div>
                    <img  src="img/pivot-num.PNG"> <br><br>
                    <p>Even though it might not contribute directly, we might get a understanding of the data like younger people have survived more and the people who paid high fare had high chances of survival. These are the things we need to make note of while building our models.</p>
                    <div class="subheading mb-3">8) Categorical Data--Bar charts to understand the categorical data</div>
                    <img  src="img/cat.PNG"> <br><br>
                    <img  src="img/cat1.PNG"> <br><br>
                    <img  src="img/cat2.PNG"> <br><br>
                    <img  src="img/cat3.PNG"> <br><br>
                    <img  src="img/cat4.PNG"> <br><br>
                    <img  src="img/cat5.PNG"> <br><br>
                    <img  src="img/cat6.PNG"> <br><br>
                    <div class="subheading mb-3">9) Comparing survival and each of these categorical variables</div>
                    <img  src="img/catsurvival.PNG"> <br><br>
                    <img  src="img/catsurvival1.PNG"> <br><br>
                    <p>From the above pivot table we can understand that relatively the lot of people from first class survived,lot of women compared to men were rescued first.We can understand that this feature(sex) will play an important contribution in the prediction.</p>
                    <div class="subheading mb-3">10) To find the percentage of the women survived in titanic</div>
                    <img  src="img/womensurvival.PNG"> <br><br>
                    <div class="subheading mb-3">11) To find the percentage of the Men survived in titanic</div>
                    <img  src="img/mensurvival.PNG"> <br><br>
                    <div class="subheading mb-3">12) Developing a model-RandomeForestClassifer model</div>
                    <p>I have used what's known as a random forest model. This model is constructed of several "trees" that will individually consider each passenger's data and vote on whether the individual survived. Then, the random forest model makes a decision based on majority: the outcome with the most votes wins.The code cell below looks for patterns in four different columns ("Pclass", "Sex", "SibSp", and "Parch") of the data. It constructs the trees in the random forest model based on patterns in the train.csv file, before generating predictions for the passengers in test.csv.</p>
                    <p>Creating the RandomeForestClassifer model to train and fit the data. After that we predict whether the passengers in the test_data had survived or not(0/1). At the end we load the predictions to prediction_submission.csv.</p>
                    <img  src="img/model.PNG"> <br><br>
                      






                    
                    


             
        

            
            
             
          </ul>
        </div>
      </section>









    </div>

    <!-- Bootstrap core JavaScript -->
    <script src="vendor/jquery/jquery.min.js"></script>
    <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="vendor/jquery-easing/jquery.easing.min.js"></script>

    <!-- Custom scripts for this template -->
    <script src="js/resume.min.js"></script>

  </body>

</html>
